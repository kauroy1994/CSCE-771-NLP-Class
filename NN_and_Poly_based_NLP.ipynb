{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Generator using Pytorch\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-7IneOv1VLDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc4LuwPPQhfg",
        "outputId": "73f89a90-1ef5-4cc6-e8b6-d4075322ab73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#Install Torch\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install PDF processing library\n",
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhaVqdWcQlwl",
        "outputId": "9f9f33e6-bcca-44aa-b0f3-1d767605c1a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CV PDF and extract text\n",
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        pages = [page.extract_text() for page in pdf.pages]\n",
        "    return ''.join(pages)\n",
        "\n",
        "pdf_path = \"about.pdf\"\n",
        "cv_text = extract_text_from_pdf(pdf_path)[:125]\n",
        "print (cv_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMm0h2QcQnfm",
        "outputId": "b84db7ba-73b9-4bac-c044-e3273dc901ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/24/24, 9:23 AM About Me - Kaushik Roy\n",
            "About Me\n",
            "I am Kaushik Roy, a Ph.D. candidate at the Artificial Intelligence\n",
            "Institute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data loading code\n",
        "chars = list(set(cv_text))\n",
        "\n",
        "class Tokenizer(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               tokens = None):\n",
        "\n",
        "    self.tokens = tokens\n",
        "    self.n_tokens = len(tokens)\n",
        "\n",
        "  def encode(self,\n",
        "             text):\n",
        "\n",
        "    text_chars = list(text)\n",
        "    return ([self.tokens.index(c) for c in text_chars])\n",
        "\n",
        "  def decode(self,\n",
        "             text_encoding):\n",
        "\n",
        "    return ''.join([self.tokens[encoding] for encoding in text_encoding])\n",
        "\n",
        "import torch\n",
        "from random import sample, shuffle\n",
        "\n",
        "class Dataloader(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               tokenizer = None,\n",
        "               text = None):\n",
        "\n",
        "    self.context_size = len(list(text))\n",
        "    X, Y = [], []\n",
        "    for t in range(self.context_size-1):\n",
        "\n",
        "      x, y = tokenizer.encode(text[:t+1]), tokenizer.encode(text[t+1])\n",
        "      X += [x]; Y += [y[0]]\n",
        "\n",
        "    self.data = list([list(item) for item in zip(X,Y)])\n",
        "\n",
        "  def get_batch(self,\n",
        "                n = None):\n",
        "\n",
        "    if n is None:\n",
        "\n",
        "      shuffle(self.data)\n",
        "      return self.data\n",
        "\n",
        "    else:\n",
        "\n",
        "      batch = sample(self.data,n)\n",
        "      return batch"
      ],
      "metadata": {
        "id": "Gp7DSXWvRAg0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#neural network code\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "class generator(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               n_tokens = None,\n",
        "               emb_size = None,\n",
        "               context_size = None,\n",
        "               n_layers = 2,\n",
        "               h_size = 100):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_tokens = n_tokens\n",
        "    self.emb_size = emb_size\n",
        "    self.context_size = context_size\n",
        "    self.n_layers = n_layers\n",
        "    self.h_size = h_size\n",
        "\n",
        "    self.embeddings = nn.Embedding(self.n_tokens, self.emb_size)\n",
        "    self.pos_embeddings = nn.Embedding(self.context_size, self.emb_size)\n",
        "\n",
        "    self.fc1 = nn.Linear(self.emb_size,self.h_size,bias=False)\n",
        "    self.fc2 = nn.Linear(self.h_size,self.h_size,bias=False)\n",
        "    self.head = nn.Linear(self.h_size,self.n_tokens)\n",
        "\n",
        "  def forward(self,\n",
        "              token_encodings):\n",
        "\n",
        "    n_tokens = len(token_encodings)\n",
        "    token_encodings = torch.tensor(token_encodings)\n",
        "    token_encodings.to(device)\n",
        "    token_embeddings = self.embeddings(token_encodings)\n",
        "    pos_embeddings = self.pos_embeddings(torch.arange(n_tokens))\n",
        "    token_embeddings += pos_embeddings\n",
        "\n",
        "    reps = token_embeddings\n",
        "    reps = F.leaky_relu(self.fc1(reps))\n",
        "    reps = F.leaky_relu(self.fc2(reps))\n",
        "    reps = self.head(reps)\n",
        "\n",
        "    logits = reps[-1]\n",
        "    return logits\n",
        "\n",
        "  def generate(self,\n",
        "               x):\n",
        "\n",
        "    for i in range(100):\n",
        "\n",
        "      x = x[:self.context_size]\n",
        "      logits = self(x)\n",
        "      logits = F.softmax(logits,dim=-1)\n",
        "      next_id = torch.multinomial(logits,num_samples = 1)\n",
        "      x = x + next_id.tolist()\n",
        "\n",
        "    return tokenizer.decode(x)\n",
        "\n",
        "\n",
        "  def train(self,\n",
        "            data_loader):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(self.parameters())\n",
        "\n",
        "    for i in tqdm(range(20)):\n",
        "\n",
        "      batch = data_loader.get_batch()\n",
        "      n_batch = len(batch)\n",
        "      loss = F.cross_entropy\n",
        "\n",
        "      batch_loss = 0.0\n",
        "      for item in batch:\n",
        "\n",
        "        x, y = item[0], item[1]\n",
        "        logits = self(x)\n",
        "        targets = [0.0]*self.n_tokens; targets[y] = 1.0\n",
        "        targets = torch.tensor(targets)\n",
        "        batch_loss += loss(logits, targets)\n",
        "\n",
        "      batch_loss /= n_batch\n",
        "      print (batch_loss.item())\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "3exvuLzuRLIQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set up trainer\n",
        "tokenizer = Tokenizer(tokens=chars)\n",
        "data_loader = Dataloader(tokenizer, cv_text)\n",
        "n_tokens = tokenizer.n_tokens\n",
        "emb_size = len(data_loader.get_batch())\n",
        "context_size = len(data_loader.get_batch())\n",
        "\n",
        "text_generator1 = generator(n_tokens = n_tokens,\n",
        "                            emb_size = emb_size,\n",
        "                            context_size = context_size)"
      ],
      "metadata": {
        "id": "Rc5lY-ltRRok"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the network\n",
        "import torch\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "text_generator1.train(data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZATjT-qhRfV8",
        "outputId": "408c4eea-4543-4960-c0e6-054b70687bf8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 2/20 [00:00<00:02,  7.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.625986099243164\n",
            "3.569924831390381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 4/20 [00:00<00:01,  8.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5167958736419678\n",
            "3.4650580883026123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [00:00<00:01,  8.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.413957118988037\n",
            "3.3625783920288086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 8/20 [00:01<00:01,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.310307025909424\n",
            "3.2565419673919678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 10/20 [00:01<00:01,  8.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.200972318649292\n",
            "3.143232583999634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 12/20 [00:01<00:00,  8.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.083127021789551\n",
            "3.020684003829956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 14/20 [00:01<00:00,  7.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9557526111602783\n",
            "2.888597249984741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 16/20 [00:02<00:00,  8.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.81943416595459\n",
            "2.7485013008117676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 18/20 [00:02<00:00,  8.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6762807369232178\n",
            "2.603827953338623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:02<00:00,  7.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5314013957977295\n",
            "2.4595391750335693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inference code\n",
        "import time\n",
        "import sys\n",
        "for o in range(1):\n",
        "\n",
        "  prompt = cv_text[:torch.randint(1,len(list(cv_text)),(1,))]\n",
        "  print ('prompt: ', prompt)\n",
        "  generated_tokens = text_generator1.generate(tokenizer.encode(prompt))\n",
        "  print ('Generating the prompt and the auto-completed text below ... ')\n",
        "  for token in generated_tokens:\n",
        "\n",
        "    sys.stdout.write(token)\n",
        "    sys.stdout.flush()\n",
        "    time.sleep(0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiBcXg-ySSx8",
        "outputId": "46149364-41a7-4aca-f371-e5cffd644cc0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt:  9/24/24, 9:23 AM About Me - Kaushik Roy\n",
            "About Me\n",
            "I am Kaushik Roy, a Ph.D. \n",
            "Generating the prompt and the auto-completed text below ... \n",
            "9/24/24, 9:23 AM About Me - Kaushik Roy\n",
            "About Me\n",
            "I am Kaushik Roy, a Ph.D. h.2lKilt s io\n",
            "KyedAyIiiti otin4tes khu,:4:, l24ank"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Generator using Pytorch"
      ],
      "metadata": {
        "id": "Vwe-ESZFVRtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#polynomial generator code\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "class moment_generator(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               n_tokens = None,\n",
        "               emb_size = None,\n",
        "               context_size = None,\n",
        "               moment_order = None):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_tokens = n_tokens\n",
        "    self.emb_size = emb_size\n",
        "    self.context_size = context_size\n",
        "    self.moment_order = moment_order\n",
        "\n",
        "    self.embeddings = nn.Embedding(self.n_tokens, self.emb_size)\n",
        "    self.pos_embeddings = nn.Embedding(self.context_size, self.emb_size)\n",
        "    self.head = nn.Linear(self.emb_size,self.n_tokens)\n",
        "\n",
        "  def forward(self,\n",
        "              token_encodings):\n",
        "\n",
        "    n_tokens = len(token_encodings)\n",
        "    token_encodings = torch.tensor(token_encodings)\n",
        "    token_encodings.to(device)\n",
        "    token_embeddings = self.embeddings(token_encodings)\n",
        "    pos_embeddings = self.pos_embeddings(torch.arange(n_tokens))\n",
        "    token_embeddings += pos_embeddings\n",
        "\n",
        "    moments = torch.row_stack([torch.mean(torch.pow(token_embeddings,k),dim=0) for k in range(self.moment_order)])\n",
        "    logits = self.head(moments)[-1]\n",
        "    return logits\n",
        "\n",
        "  def generate(self,\n",
        "               x):\n",
        "\n",
        "    for i in range(100):\n",
        "\n",
        "      x = x[:self.context_size]\n",
        "      logits = self(x)\n",
        "      logits = F.softmax(logits,dim=-1)\n",
        "      next_id = torch.multinomial(logits,num_samples = 1)\n",
        "      x = x + next_id.tolist()\n",
        "\n",
        "    return tokenizer.decode(x)\n",
        "\n",
        "  def train(self,\n",
        "            data_loader):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(self.parameters())\n",
        "\n",
        "    acc_loss = None\n",
        "    for i in range(20):\n",
        "\n",
        "      batch = data_loader.get_batch()\n",
        "      n_batch = len(batch)\n",
        "      loss = F.cross_entropy\n",
        "\n",
        "      batch_loss = 0.0\n",
        "      for item in batch:\n",
        "\n",
        "        x, y = item[0], item[1]\n",
        "        logits = self(x)\n",
        "        targets = [0.0]*self.n_tokens; targets[y] = 1.0\n",
        "        targets = torch.tensor(targets)\n",
        "        batch_loss += loss(logits, targets)\n",
        "\n",
        "      batch_loss /= n_batch\n",
        "      acc_loss = batch_loss\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    print (acc_loss.item())"
      ],
      "metadata": {
        "id": "Ky9WabBiTz82"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#polynomial function trainer setup\n",
        "tokenizer = Tokenizer(tokens=chars)\n",
        "data_loader = Dataloader(tokenizer, cv_text)\n",
        "n_tokens = tokenizer.n_tokens\n",
        "emb_size = len(data_loader.get_batch())\n",
        "context_size = len(data_loader.get_batch())\n",
        "\n",
        "text_generator2 = moment_generator(n_tokens = n_tokens,\n",
        "                                   emb_size = emb_size,\n",
        "                                   context_size = context_size,\n",
        "                                   moment_order = 3)"
      ],
      "metadata": {
        "id": "zUJsTSuLT8vN"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#polynomial function training\n",
        "from tqdm import tqdm\n",
        "for k in tqdm(range(20)):\n",
        "  text_generator2.train(data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NYNLnIcUCS4",
        "outputId": "0f43fcaf-d2b5-4436-da60-86d30e226d49"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 1/20 [00:02<00:40,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1688625812530518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [00:04<00:39,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.867598533630371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [00:06<00:36,  2.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6762983798980713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [00:08<00:34,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5302839279174805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [00:11<00:37,  2.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4144108295440674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [00:13<00:33,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3230040073394775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [00:16<00:30,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2442479133605957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [00:18<00:27,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1711087226867676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [00:20<00:24,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.104647159576416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [00:22<00:23,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.041882038116455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [00:25<00:22,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.981392502784729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [00:27<00:19,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.925990343093872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [00:30<00:16,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.874157428741455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [00:32<00:13,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8272738456726074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [00:34<00:11,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7830777168273926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [00:37<00:09,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.74057936668396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [00:39<00:07,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6986557245254517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [00:41<00:04,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6586095094680786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [00:43<00:02,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6233717203140259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:46<00:00,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5911303758621216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inference code\n",
        "import time\n",
        "import sys\n",
        "for o in range(1):\n",
        "\n",
        "  prompt = cv_text[:torch.randint(1,len(list(cv_text)),(1,))]\n",
        "  print ('prompt: ', prompt)\n",
        "  generated_tokens = text_generator2.generate(tokenizer.encode(prompt))\n",
        "  print ('Generating the prompt and the auto-completed text below ... ')\n",
        "  for token in generated_tokens:\n",
        "\n",
        "    sys.stdout.write(token)\n",
        "    sys.stdout.flush()\n",
        "    time.sleep(0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrGjdIwQUsbC",
        "outputId": "7065c6dc-a755-4ea5-8483-5b15c4d26f13"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt:  9/24/24, 9:23 AM About Me - Kaushik Roy\n",
            "About Me\n",
            "I am Kaushik \n",
            "Generating the prompt and the auto-completed text below ... \n",
            "9/24/24, 9:23 AM About Me - Kaushik Roy\n",
            "About Me\n",
            "I am Kaushik k  aoPh h.RRh,o h eahRia ct l  fatsacIihtini eecIiseiieh netao\n"
          ]
        }
      ]
    }
  ]
}