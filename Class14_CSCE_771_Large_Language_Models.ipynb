{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Large Language Models Example**"
      ],
      "metadata": {
        "id": "GxYnkJy2ohcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeqEaBJOmRRk",
        "outputId": "d3cec866-280d-4f6c-8bea-2f775494b43a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jblzxjWaeFdV"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "\n",
        "class PromptTemplates:\n",
        "\n",
        "    @staticmethod\n",
        "    def get_CV_summarization_template(cv_text,job_role=\"Research Scientist\"):\n",
        "\n",
        "        return f\"\"\"Consider the CV_text below:\n",
        "\n",
        "        ---- CV Text ----\n",
        "\n",
        "        {cv_text}\n",
        "\n",
        "        Consider also the job role below:\n",
        "\n",
        "        --- Job Role ----\n",
        "\n",
        "        {job_role}\n",
        "\n",
        "        Use the cv_text and the job role provided and respond with a summary of the CV that helps a recruiter make a decision\n",
        "        Make sure to format your response in a readable format,\n",
        "        and only provide the answer without any extra information\n",
        "        \"\"\"\n",
        "\n",
        "class LLM:\n",
        "\n",
        "    def __init__(self, api=None, groq_model=None, temperature=None):\n",
        "\n",
        "        self.api = 'GROQ'\n",
        "        self.groq_model = \"mixtral-8x7b-32768\"\n",
        "        self.temperature = 0.0\n",
        "        self.groq_client = Groq(api_key=\"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\")\n",
        "\n",
        "    def set_prompt(self, cv_text, prompt=None):\n",
        "\n",
        "        if prompt is None:\n",
        "          self.prompt = PromptTemplates.get_CV_summarization_template(cv_text)\n",
        "          return\n",
        "\n",
        "        self.prompt = prompt\n",
        "\n",
        "    def respond_to_prompt(self):\n",
        "        prompt = self.prompt\n",
        "\n",
        "        if self.api == 'GROQ':\n",
        "            client = self.groq_client\n",
        "\n",
        "            # Create chat completion with configured model and temperature\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt,\n",
        "                    }\n",
        "                ],\n",
        "                temperature=self.temperature,\n",
        "                model=self.groq_model,\n",
        "            )\n",
        "\n",
        "            llm_response = str(chat_completion.choices[0].message.content)\n",
        "            return llm_response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample CV text\n",
        "cv_text = \"\"\"\n",
        "Experienced data scientist with expertise in machine learning, natural language processing, and AI applications.\n",
        "Proficient in Python, TensorFlow, and cloud computing. Published in top-tier AI journals and conferences.\n",
        "Skilled in data analysis, statistical modeling, and developing end-to-end AI pipelines.\n",
        "\"\"\"\n",
        "\n",
        "resume_llm = LLM()\n",
        "resume_llm.set_prompt(cv_text)\n",
        "resume_llm.respond_to_prompt()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "8Sg_kls7g0g4",
        "outputId": "f8903816-08b0-4bea-c561-1721e884b2e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary of CV:\\n\\nThe CV belongs to an experienced data scientist with expertise in machine learning, natural language processing, and AI applications. They are proficient in Python, TensorFlow, and cloud computing, and have published in top-tier AI journals and conferences. Their skills include data analysis, statistical modeling, and developing end-to-end AI pipelines. These qualifications make them a strong candidate for the Research Scientist position, where they can utilize their expertise in AI and machine learning to conduct research and develop innovative solutions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instruction Tuning Example**\n",
        "\n",
        "[Notebook Link](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb#scrollTo=IQ1sMda27Zj6)\n",
        "\n",
        "## Required hardware\n",
        "\n",
        "The notebook is designed to be run on any NVIDIA GPU which has the [Ampere architecture](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) or later with at least 24GB of RAM. This includes:\n",
        "\n",
        "* NVIDIA RTX 3090, 4090\n",
        "* NVIDIA A100, H100, H200\n",
        "\n",
        "and so on. Personally I'm running the notebook on an RTX 4090 with 24GB of RAM.\n",
        "\n",
        "The reason for an Ampere requirement is because we're going to use the [bfloat16 (bf16) format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), which is not supported on older architectures like Turing.\n",
        "\n",
        "But: a few tweaks can be made to train the model in float16 (fp16), which is supported by older GPUs like:\n",
        "\n",
        "* NVIDIA RTX 2080\n",
        "* NVIDIA Tesla T4\n",
        "* NVIDIA V100."
      ],
      "metadata": {
        "id": "OZk_m0Ylon7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tune Gemma models in Keras using LoRA**\n",
        "\n",
        "[Notebook link](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb#scrollTo=SDEExiAk4fLb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models.\n",
        "\n",
        "Large Language Models (LLMs) like Gemma have been shown to be effective at a variety of NLP tasks. An LLM is first pre-trained on a large corpus of text in a self-supervised fashion. Pre-training helps LLMs learn general-purpose knowledge, such as statistical relationships between words. An LLM can then be fine-tuned with domain-specific data to perform downstream tasks (such as sentiment analysis).\n",
        "\n",
        "LLMs are extremely large in size (parameters in the order of billions). Full fine-tuning (which updates all the parameters in the model) is not required for most applications because typical fine-tuning datasets are relatively much smaller than the pre-training datasets.\n",
        "\n",
        "[Low Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685) is a fine-tuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights of the model and inserting a smaller number of new weights into the model. This makes training with LoRA much faster and more memory-efficient, and produces smaller model weights (a few hundred MBs), all while maintaining the quality of the model outputs.\n",
        "\n",
        "This tutorial walks you through using KerasNLP to perform LoRA fine-tuning on a Gemma 2B model using the [Databricks Dolly 15k dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k). This dataset contains 15,000 high-quality human-generated prompt / response pairs specifically designed for fine-tuning LLMs."
      ],
      "metadata": {
        "id": "yReXIxLPui0M"
      }
    }
  ]
}